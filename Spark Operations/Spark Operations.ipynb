{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "380a1b7b-14b7-470c-90ba-8969d2f97219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Operations\n",
    "### Lesson objectives\n",
    "#### In this lesson, we will explain the following topics:\n",
    "\n",
    "- Understand the two types of Spark operations: transformations and actions.\n",
    "- Learn about the immutability of Spark operations and its implications.\n",
    "- Explore examples of transformations and actions, including lazy evaluation and its benefits.\n",
    "### Spark Operations\n",
    "#### Spark Operations\n",
    "- Spark operations on distributed data can be classified into two types: transformations and actions.\n",
    "- All spark operations are immutable.\n",
    "### Immutable Objects\n",
    "- An object whose state cannot change after it has been constructed is called immutable (unchangeable).1\n",
    "- The methods of an immutable object do not modify the state of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67dac3b0-3a15-4ed0-9d76-807ed110641d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data RDD:\n('Abdo', 25)\n('Ola', 23)\n('Esraa', 28)\n('Asmaa', 32)\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Abdo\",25),(\"Ola\", 23),(\"Esraa\", 28),(\"Asmaa\",32)]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "print(\"Original Data RDD:\")\n",
    "for row in rdd.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef1e137-e059-45ea-bdff-1a19178d3f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original RDD ID: 6\nRDD Filter:\n('Abdo', 25)\n('Esraa', 28)\n('Asmaa', 32)\nRDD ID: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original RDD ID: {rdd.id()}\")\n",
    "\n",
    "rdd_filter = rdd.filter(lambda x : x[1] > 23)\n",
    "print(\"RDD Filter:\")\n",
    "for row in rdd_filter.collect():\n",
    "    print(row)\n",
    "\n",
    "print(f\"RDD ID: {rdd_filter.id()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc2e567a-e366-469c-b04a-9f22a6b42b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark Operations: Transformations\n",
    "- Transformations: transform a Spark DataFrame into a new DataFrame without altering the original data.\n",
    "- Example of Spark transformations: **map(), select(), filter(), or drop()**.\n",
    "\n",
    "### Spark Transformations: What are Lazy Transformations?\n",
    "- In Spark, transformations are lazy.\n",
    "- This means computations are not executed immediately.\n",
    "- Spark builds a DAG (Directed Acyclic Graph) of transformations.\n",
    "- All Transformations results are not computed immediately, but they are recorded or remembered as a lineage.\n",
    "### Spark Transformations: Benefits of Lazy Evaluation\n",
    "- Optimization: A lineage allows Spark, at a later time in its execution plan, to rearrange certain transformations, coalesce them, or optimize transformations into stages for more efficient execution.\n",
    "- Resource Management: Executes tasks efficiently, using fewer resources.\n",
    "- Fault Tolerance: Easier to recompute parts of the pipeline if a part fails.\n",
    "### Spark Transformations: Lazy Transformation\n",
    "- Consider a dataset with map and filter transformations.\n",
    "- Spark does not execute these transformations when they are defined.\n",
    "- Transformations are executed when an action (like collect, count) is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9eae06e-c0a3-4980-b55e-dbfb66cbeb6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Age</th></tr></thead><tbody><tr><td>Abdo</td><td>25</td></tr><tr><td>Esraa</td><td>28</td></tr><tr><td>Asmaa</td><td>32</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Abdo",
         25
        ],
        [
         "Esraa",
         28
        ],
        [
         "Asmaa",
         32
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"Abdo\",25),(\"Ola\", 23),(\"Esraa\", 28),(\"Asmaa\",32)]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "\n",
    "map_rdd = rdd.map(lambda x: (x[0], x[1], x[1]>23))\n",
    "\n",
    "filter_rdd = map_rdd.filter(lambda x : x[2])\n",
    "\n",
    "df = spark.createDataFrame(filter_rdd , ['Name' , \"Age\" , \" OlderThan23\"])\n",
    "\n",
    "final_df = df.select('Name' , \"Age\")\n",
    "\n",
    "result = final_df.collect()\n",
    "\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3c1685f-4205-404f-a588-60c2b1949f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark Operations: Actions\n",
    "- An action triggers the lazy evaluation of all the recorded transformations.\n",
    "- Actions are operations that trigger execution of transformations.\n",
    "- They are used to either compute a result to be returned to the Spark driver program or to write data to an external storage system.\n",
    "- Actions include operations like **count, collect, saveAsTextFile, and take**.\n",
    "### Examples of Spark Actions\n",
    "- **collect()**: Collects all elements from the Spark context to the driver program.\n",
    "- **count()**: Returns the number of elements in the dataset.\n",
    "- **saveAsTextFile(path)**: Saves the dataset to a text file at the specified path.\n",
    "- **take(n)**: Returns an array with the first n elements of the dataset."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Operations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
